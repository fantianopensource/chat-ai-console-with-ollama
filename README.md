# LocalAI

A local AI chat program based on .NET, supporting conversation with local LLMs via the Ollama API.

## Features

- Interactive console chat
- Streaming output
- Customizable model and API endpoint

## Quick Start

1. Make sure Ollama is running locally and the target model (e.g., gemma3n:e4b) is started.
2. Clone this project and enter the directory:
   ```sh
   git clone git@github.com:fantianopensource/chat-ai-console-with-ollama.git
   cd LocalAI
   ```
3. Build and run:
   ```sh
   dotnet run
   ```
4. Enter your prompt as instructed and chat with the local AI.

## Dependencies

- .NET 9.0
- OllamaSharp

## License

MIT
